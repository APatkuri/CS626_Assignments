{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/aditya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package semcor to /home/aditya/nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditya/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/aditya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tree.tree import Tree\n",
    "from nltk.corpus import semcor\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download(['wordnet', 'semcor', 'omw-1.4', 'stopwords'])\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "count = 0\n",
    "for sents in semcor.tagged_sents(tag=\"sem\"):\n",
    "  x = []\n",
    "  y = []\n",
    "  for w in sents:\n",
    "    if isinstance(w, Tree) and isinstance(w[0], str):\n",
    "      y.append(w.label())\n",
    "      x.append(w[0])\n",
    "  x_train.append(x)\n",
    "  y_train.append(y)\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for w in semcor.words():\n",
    "  corpus.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(sentence_data_for_vocab, num_most_freq_words=20000, max_word_length=50):\n",
    "  vectorizer = TextVectorization(max_tokens=num_most_freq_words, output_sequence_length=max_word_length, standardize='lower')\n",
    "  text_ds = tf.data.Dataset.from_tensor_slices(sentence_data_for_vocab).batch(128)\n",
    "  vectorizer.adapt(text_ds)\n",
    "  voc = vectorizer.get_vocabulary()\n",
    "  word_index = dict(zip(voc, range(len(voc))))\n",
    "  return vectorizer, voc, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 00:18:14.049939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-27 00:18:14.050200: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-27 00:18:14.050245: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-27 00:18:14.050280: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-27 00:18:14.050315: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-11-27 00:18:14.050353: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-11-27 00:18:14.050388: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-27 00:18:14.050423: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-27 00:18:14.050459: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-11-27 00:18:14.050466: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-27 00:18:14.051886: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "vectorizer, voc, word_index = create_vectorizer(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NUM_WORDS = 20000\n",
    "word_index = word_index\n",
    "num_words = len(voc)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = embeddings_index.get_vector(word)\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        count += 1\n",
    "    except (KeyError):\n",
    "        continue\n",
    "        \n",
    "embedding_layer_google = tf.keras.layers.Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=50,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_cnt = 0\n",
    "r_cnt = 0\n",
    "num = 0\n",
    "for idx in range(len(x_train)):\n",
    "  if idx==1:\n",
    "    break\n",
    "  x = x_train[idx]\n",
    "  y = y_train[idx]\n",
    "  g = nx.Graph()\n",
    "  defs = []\n",
    "  vecs = []\n",
    "  try:\n",
    "    num += len(x)\n",
    "    for word in x:\n",
    "      t = np.array([[sns.definition()] for sns in wordnet.synsets(word)])\n",
    "      vecs_loc = embedding_layer_google(vectorizer(t))\n",
    "      vecs_loc = tf.keras.utils.normalize(tf.reduce_sum(vecs_loc, 1))\n",
    "      defs.append([(sns.definition(), sns.lemmas()) for sns in wordnet.synsets(word)])\n",
    "      vecs.append(vecs_loc)\n",
    "    for w in defs:\n",
    "      for d in w:\n",
    "        g.add_node(d[0])\n",
    "    for idx in range(len(defs)-1):\n",
    "      w1 = defs[idx]\n",
    "      w2 = defs[idx+1]\n",
    "      for id1 in range(len(w1)):\n",
    "        for id2 in range(len(w2)):\n",
    "          weight = tf.tensordot(vecs[idx][id1], vecs[idx+1][id2], 1)\n",
    "          g.add_edge(defs[idx][id1][0], defs[idx+1][id2][0], weight=tf.keras.backend.get_value(weight))\n",
    "    # nx.draw(g)\n",
    "    pr = nx.pagerank(g)\n",
    "    senses = []\n",
    "    for w in defs:\n",
    "      max_pr = 0\n",
    "      sense = None\n",
    "      for d in w:\n",
    "        if pr[d[0]] > max_pr:\n",
    "          max_pr = pr[d[0]]\n",
    "          sense = d[1]\n",
    "      senses.append(sense)\n",
    "    for idx in range(len(x)):\n",
    "      if y[idx] in senses[idx]:\n",
    "        p_cnt += 1\n",
    "  except:\n",
    "    pass\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy =\",p_cnt/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'he goes to a bank to deposit his money'\n",
    "\n",
    "def sentTokenize(str):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "senttoken = sentTokenize(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeunnecessarywords(sentences2):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    context_tab=[]\n",
    "    for sentence in sentences2:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        without_stop_words = [word for word in words if not word in stop_words]\n",
    "        context_tab.append(without_stop_words)\n",
    "    return context_tab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['goes', 'bank', 'deposit', 'money']\n"
     ]
    }
   ],
   "source": [
    "remuness = removeunnecessarywords(senttoken)\n",
    "testx = [item for sublist in  remuness for item in sublist]\n",
    "print(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('belong.v.03.belong'), Lemma('belong.v.03.go')]\n",
      "[Lemma('deposit.v.02.deposit'), Lemma('deposit.v.02.bank')]\n",
      "[Lemma('deposit.v.02.deposit'), Lemma('deposit.v.02.bank')]\n",
      "[Lemma('money.n.03.money')]\n"
     ]
    }
   ],
   "source": [
    "g = nx.Graph()\n",
    "defss = []\n",
    "vecs = []\n",
    "pt = 0\n",
    "for word in testx:\n",
    "  t = np.array([[sns.definition()] for sns in wordnet.synsets(word)])\n",
    "  # print(word, t)\n",
    "  try:\n",
    "    vecs_loc = embedding_layer_google(vectorizer(t))\n",
    "    vecs_loc = tf.keras.utils.normalize(tf.reduce_sum(vecs_loc, 1))\n",
    "  except:\n",
    "    print(\" \")\n",
    "  defss.append([(sns.definition(), sns.lemmas()) for sns in wordnet.synsets(word)])\n",
    "  vecs.append(vecs_loc)\n",
    "for w in defss:\n",
    "  for d in w:\n",
    "    g.add_node(d[0])\n",
    "for idx in range(len(defss)-1):\n",
    "  w1 = defss[idx]\n",
    "  w2 = defss[idx+1]\n",
    "  for id1 in range(len(w1)):\n",
    "    for id2 in range(len(w2)):\n",
    "      weight = tf.tensordot(vecs[idx][id1], vecs[idx+1][id2], 1)\n",
    "      g.add_edge(defss[idx][id1][0], defss[idx+1][id2][0], weight=tf.keras.backend.get_value(weight))\n",
    "# nx.draw(g)\n",
    "pr = nx.pagerank(g)\n",
    "senses = []\n",
    "for w in defss:\n",
    "  max_pr = 0\n",
    "  sense = None\n",
    "  for d in w:\n",
    "    if pr[d[0]] > max_pr:\n",
    "      max_pr = pr[d[0]]\n",
    "      sense = d[1]\n",
    "  senses.append(sense)\n",
    "for i in senses:\n",
    "  print(i)\n",
    "assert len(testx) == len(senses)\n",
    "# for idx in range(len(testx)):\n",
    "#   print(testx[idx],\":\",senses[idx])\n",
    "#   if testy[idx] in senses[idx]:\n",
    "#     pt += 1\n",
    "  \n",
    "# print(\"Accuracy: \",pt/len(testx) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
